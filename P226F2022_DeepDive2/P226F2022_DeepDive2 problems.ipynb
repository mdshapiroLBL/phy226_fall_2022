{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Physcs 226 Fall 2022 DeepDive2: Likelihood Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff4a2b2",
   "metadata": {},
   "source": [
    "The likelihood ${\\cal L}(x;\\theta$ is probability that a measurement of x will yield a specific value for a given theory.  To determine the likelihood, you must know both the theory and the values of any parameters the theory depends on.   If we have an ensemble of measurements, overall likelihood\n",
    "obtained from product of the likelihoods for the measurements\n",
    "\n",
    "$$\n",
    "{\\cal L}(x;\\theta) = \\prod_{i=1}^n {\\cal L}_i (x_i;\\theta)\n",
    "$$\n",
    "\n",
    "Here θ can represent one or more parameters.\n",
    "\n",
    "Likelihood functions can be used to estimate the parameters $\\theta$.  To do this, you wand to maximize the likelihood.  Since products are a pain to deal with conputationally, usually it is more convenient to maximize the log of the likehood\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\frac{\\partial \\ln {\\cal L}}{\\partial  \\theta} &= &\\frac{\\partial} {\\partial \\theta} \\ln \\prod_{i=1}^n {\\cal L}_i   \\\\\n",
    "& = & \\frac{\\partial}{\\partial \\theta} \\sum_{i=1}^n \\ln {\\cal L}_i \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The  aximum of likelihood function corresponds to value of $\\theta$  that min\\imizes $- \\sum_i\\ln {\\cal L}_i $.   If there ae several $\\theta$ parameters, we can minimize with respect to each one.\n",
    "\n",
    "As an example:  suppose we have a process where the number of events is described by a Poisson distribution with mean number $\\mu$ amd we measure this process with $N$ independent trials with measurements $n_i$ (where $n_i$ are integers). The likelihood for a single trial follows the Poisson distribution\n",
    "\n",
    "$$                                                                              \n",
    "{\\cal L}(n_i;\\mu) = \\frac{e^{-\\mu}(\\mu)^n}{n!}                              \n",
    "$$\n",
    "\n",
    "and the overall likehood for all the trials is the product of the $N$ likelhoods.  Then:\n",
    "\n",
    "$$\n",
    "\\begin{array}{{rcl}\n",
    "\\cal L}(data;\\mu) & = & \\prod_{i=1}^N  \\frac{ e^{-\\mu} (\\mu)^{n_i}}{n_i!}\\\\\n",
    "\\ln {\\cal L} & = & \\sum_i\n",
    "\\left (  -\\mu + n_i\\ln \\mu\n",
    "-\\ln (n_i!)\n",
    "\\right )  \\\\\n",
    "   & = & -N\\mu + \\left ( \\sum_i n_i \\right )\\ln \\mu + constant \\\\\n",
    "\\frac{\\partial {\\ln \\cal L}}{d\\mu }|_{\\hat\\mu=\\mu} & = & -N + \\frac{\n",
    "\\sum_i n_i}{\\mu} = 0 \\\\\n",
    "\\hat \\mu & = & \\frac{1}{N} \\sum_{i=1}^N n_i \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "As expected, the best estimator is the mean value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd19d40b",
   "metadata": {},
   "source": [
    "##  Question 1:  LogLikelihood and $\\chi^2$\n",
    "\n",
    "### Learning objectives\n",
    "In this question you will:\n",
    "\n",
    "- Relate the log of the likelihood function for a Gaussian pdf to the $\\chi^2$ distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bc6970",
   "metadata": {},
   "source": [
    "We say in Homework \\#3 that the distribution of the sum (or average) of a large number of independent, identicallyistributed measurements will be approximately normal, regardless of the underlying distribution.  That means in the limit of high statistics, (for example a histogram where the number of events in each bin is large), the probability of seeing $n_i$ events in bin $i$ will be Gaussianly distributed:\n",
    "\n",
    " $$                                                                            \n",
    " G(x|\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \n",
    " $$\n",
    "\n",
    "### 1a.\n",
    "\n",
    "Consider the case where we make $N$ idependent measurements $x_i$.for a quantiy $x$ that is Gaussianly distributed.\n",
    "Take the derivative the the log likelihood function and show that the miniumum log likehood is the mean value of $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cfc5c3",
   "metadata": {},
   "source": [
    "write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75d8eb7",
   "metadata": {},
   "source": [
    "### 1b."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f944a2",
   "metadata": {},
   "source": [
    "Show that for the case of a Gaussian likehood function\n",
    "\n",
    "$$\n",
    "- 2 \\ln {\\cal L} = \\chi^2\n",
    "$$\n",
    "\n",
    "where $\\chi^2$ has the standard definition:\n",
    "\n",
    "$$\n",
    "\\chi^2 = \\sum_{i=1}^N \\frac{(x_i-\\mu)^2}{\\sigma^2}   \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Question 2: Likelihood functions for a decay model with background and maximum time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "In this question you will:\n",
    "\n",
    "- Learn about log-likelihood fits using an example probability density function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "Consider the problem of determining the lifetime of a species of particle that we can stop in our detector by observing it decays in the presence of a constant background. The rate is given by\n",
    "$$\n",
    "R(t) = A + Be^{-t/\\tau}\n",
    "$$\n",
    "We'll take as the true parameters $A=1/$sec, $B=2/$sec, and $\\tau= 0.5$sec. Each time we stop a particle in our detector, we will only wait to see if it decays for a maximum time $t_{max}=3$ seconds. We can imagine doing the experiment over many times (each experiment takes three seconds) to accumulate a lot of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### 2a. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "First, let's generate some fake data. We will generate this data using the Monte Carlo technique known  as the *acceptance-rejection method* or *rejection sampling.* Suppose you want to generate events whose distribution follows the function $f(x)$ (here $f(x)$ is the probability distribution function). This can be achieved by generating points $(x_i,y_i)$ randomly in a two-dimensional space and keeping only the subset of the events where $y_i \\le f(x_i)$.  A more detailed explaination of the method can be found [here](https://en.wikipedia.org/wiki/Rejection_sampling).\n",
    "\n",
    "We will generate fake data in the following way.  The maximum rate is at $t=0$, where it is $R_{max}=(A+B)$.   Pick an $x_i$, the time $t$ of the decay, randomly  between 0 and $t_{max}$, and pick $y_i$ randomly between 0 and $R_{max}$. Use $R(t)$  to you decide whether to keep this event. Generate a reasonable amount of such fake data. What percentage of the time do you expect to keep an event? Is that what you find?\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### 2b. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "Calculate the negative log-likelihood function, $-\\ln {\\cal L}$, for your data taking care to use a PDF normalized so that\n",
    "\n",
    "$$\n",
    "\\int_0^{t_{max}}f(t)dt=1\n",
    "$$\n",
    "\n",
    "Using this condition, express your likelihood in terms of two  free parameters, $\\tau$ and $\\kappa=A/B$.  (Note:  you can\n",
    "see an example of how this works in the notebook t *Some Comments on Likelihood Functions*).\n",
    "\n",
    "We will study the simulated data, pretending that we dont know what  values of $A$, $B$ and $\\tau$ were used to generate it.  We want\n",
    "to determine $\\tau$ from the data. \n",
    "\n",
    "Write code to calculate the negative log-likelihood:\n",
    "\n",
    "$$\n",
    "- \\ln {\\cal L} = - \\sum_i \\ln f(\\kappa, \\tau, t_i)\n",
    "$$\n",
    "\n",
    "where the $t_i$ are the time values you accepted in the Monte Carlo process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5410e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3723a1d2",
   "metadata": {},
   "source": [
    "### 2c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce49ffee",
   "metadata": {},
   "source": [
    "There are lots of algorithms for finding the minimum of a non-linear function such as our negative log-likelihood, but we won't bother to use any of these algorithms here. Instead, we will explore the minimum by inspecting the behavour of the function. Plot (or just show the values in a 2-dimensional grid) the value of −ln you obtain from your simulated data as you vary $\\kappa$  and $\\tau$ in the region of the true answer ($\\kappa=0.5$, $\\tau=0.5$). How close is the $\\tau$ that gives minimum negative log-likelihood to the true value of Γ? Perform this test with a sample of $N=1000$ signal plus background events and $N=10000$ signal and background events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f071e7b2",
   "metadata": {},
   "source": [
    "### 2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f34d26",
   "metadata": {},
   "source": [
    "You proved in problem 1b that for high statistics $-2\\ln {\\cal L}$ is distributed like a $\\chi^2$ distribution. The the uncertainty on the estimate of a parameter of the function can be obtained by finding how much the you can change the parameter to increase $-2\\ln {\\cal L}$ by 1.  In principle, because our likelihood has 2 parameters, the uncertainty on $\\tau$ and $\\kappa$ are correlated.  However, looking at your 2D plot from part 2c, you will see that the correlation between these parameters is in fact small.  We will ignore it here.  \n",
    "\n",
    "\n",
    "Using the value of $\\kappa$ obtained from the graph in part 2c, plot  $-2\\ln {\\cal L}$ for values of $\\Gamma$ around the maximum likelihood and indicate the uncertainty on the measured value of $\\tau$.  Do this for both the $N=1000$ and $N=10000$ event cases you studied in 2c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5382b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abf14a5",
   "metadata": {},
   "source": [
    "## Question 3:  Likelihood ratios and hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b9509",
   "metadata": {},
   "source": [
    "One important use of statistics is to test hypothesis.  Suppose we want to determine whether our data is more consistent with hypothesis, $H_0$ or hypothesis, $H_1$, where the probability distribution functions are know for both hypotheses.   The Neyman-Pearson Lemma says that the region $W$ that minimizing the probability of wrongly accepting hypothesis $H_0$ is a contour in the likelihood ratio\n",
    "\n",
    "$$\n",
    "\\frac{{\\cal L}(x|H_1)}{{\\cal L}(x|H_0)} > \\kappa_\\alpha\n",
    "$$\n",
    "\n",
    "where $1-\\alpha$ is the probability for $H_0$ to be within the contour.  This Lemma and its proof are discussed in Kyle Crammer's 2020 Summer School Lecture https://indico.fnal.gov/event/43762/contributions/192672/attachments/132731/163512/HCPSS-stats-lectures-2020.pdf\n",
    "\n",
    "An example of the use of this likelihood ratio is an early LHC analysis from CMS that used the Higgs decay to 4 leptons to determine if the observed Higgs boson is indeed a scalar (Parity=+1)  or it is is instead a pseudoscalar (Parity=-1)\n",
    "\n",
    "\n",
    "<img src=\"cms-higgsParity.png\" alt=\"Drawing\" style=\"width:550px;\"/>\n",
    "\n",
    "\n",
    "This same idea can be used to determine whether the data are consistent with a bavkground only hypothesis ($H_0$)  rather than a signal-plus-background hypothesis ($H_1$).  In this case, $H_0$ is often called the NULL hypothesis\n",
    "\n",
    "We will explore this idea using the Gaussian signal plus flat background model we studied in Problem Set \\#5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a98fe4e",
   "metadata": {},
   "source": [
    "### 3a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb6f783",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Generate 1000 independent data samples with total of 550 events with a signal-to-background ratio of 0.1 (see Problem Set \\#5 for the code to do this and the definition of signal-to-background).  Also generate 1000 independent samples consisting of 550 events of pure background.  Use these samples to make a plot similar to the CMS plot shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79182aa0",
   "metadata": {},
   "source": [
    "### 3b. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca03d163",
   "metadata": {},
   "source": [
    "Repeat the experiment keeping the same signal-to-background ratio but increasing the number of events to 1000 a d then 10000.  Describe what you see in these plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d251508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa386513",
   "metadata": {},
   "source": [
    "### 3c."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42505a4",
   "metadata": {},
   "source": [
    "How many events would you need to collect so that for 99\\%\\ of the time you could rule at the NULL hypothesis at 95\\% confidence level?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d71800",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
